{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "augmented_feedforward.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgg73iS_hvOh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "outputId": "8d7e3fc8-56c5-4ddd-bfed-7753208e0579"
      },
      "source": [
        "! pip install -q tensorflow numpy pandas scikit-learn mlxtend dataprep transformers\n",
        "! cp drive/My\\ Drive/Colab\\ Notebooks/*.csv ./\n",
        "! cp drive/My\\ Drive/Colab\\ Notebooks/*.pkl ./"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 112kB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 7.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 6.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 9.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 20.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 21.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 829kB 31.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 512kB 50.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.6MB 44.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4MB 48.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.5MB 21.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 49.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 50.3MB/s \n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/1a/95/f50352b5366e7d579e8b99631680a9e32e1b22adfa1629a8f23b1d22d5e2/multidict-4.7.6-cp36-cp36m-manylinux1_x86_64.whl\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 153kB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 266kB 8.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 655kB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 6.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 7.9MB/s \n",
            "\u001b[?25h  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for locket (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: panel 0.9.7 has requirement bokeh>=2.1, but you'll have bokeh 2.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: nbclient 0.5.0 has requirement jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 5.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: distributed 2.26.0 has requirement cloudpickle>=1.5.0, but you'll have cloudpickle 1.3.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alG5ImZ0C0VV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augment(training):\n",
        "  training = np.repeat(training, 10, axis=0)\n",
        "  temp = pickle.load(open('alexnet_embeddings.pkl', 'rb'))\n",
        "  for i in range(0, len(training), 10):\n",
        "    for j in range(len(X)):\n",
        "        if np.all(training[i][:1000] == X[j][:1000]):\n",
        "          for k in range(1, 10):\n",
        "            training[i+k] = np.concatenate([temp[j*10+k][:1000], training[i+k][1000:]])\n",
        "          break\n",
        "  return training"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5VCOTGzyv1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ds_from_df(df, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('Meme')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWH9P4UkyuR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_map():\n",
        "  temp = df[['Visual']].values\n",
        "  counter = 0\n",
        "  map = {}\n",
        "  for i in temp:\n",
        "    for j in i[0].split():\n",
        "      if j.strip(',') not in map and j.strip(',') != '0':\n",
        "        map[j.strip(',')] = counter\n",
        "        counter += 1\n",
        "  return map"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj-N63Q-0rZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_X(temp):\n",
        "  res = []\n",
        "  for i in range(0, len(temp), 10):\n",
        "    res.append(temp[i])\n",
        "  return np.array(res)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djLoSoayytuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ohe(map):\n",
        "  one_hot_encode = []\n",
        "  temp = df[['Visual']].values\n",
        "\n",
        "  for i in temp:\n",
        "    arr = list(np.zeros(len(map),dtype=int))\n",
        "    for j in i[0].split():\n",
        "      if j.strip(',') != '0':\n",
        "        arr[map[j.strip(',')]] = 1\n",
        "    one_hot_encode.append(arr)\n",
        "  return np.array(one_hot_encode)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNKb12FA9zPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(input_shape=(None, 2889)):\n",
        "  model = tf.keras.Sequential([tf.keras.layers.Dense(1024, \n",
        "                                                     name='dense_1',\n",
        "                                                     activation=tf.keras.activations.relu),\n",
        "                               tf.keras.layers.Dropout(rate=0.3),\n",
        "                               tf.keras.layers.Dense(512, \n",
        "                                                     name='dense_2',\n",
        "                                                     activation=tf.keras.activations.relu),\n",
        "                               tf.keras.layers.Dense(512, \n",
        "                                                     name='dense_3',\n",
        "                                                     activation=tf.keras.activations.relu),\n",
        "                               tf.keras.layers.Dense(128, \n",
        "                                                     name='dense_4',\n",
        "                                                     activation=tf.keras.activations.relu),\n",
        "                               tf.keras.layers.Dense(64, \n",
        "                                                     name='dense_5',\n",
        "                                                     activation=tf.keras.activations.relu),\n",
        "                               tf.keras.layers.Dense(1, name='output',\n",
        "                                                     activation=tf.keras.activations.sigmoid)])\n",
        "  print(model.compute_output_shape(input_shape=input_shape))\n",
        "  model.build(input_shape=input_shape)\n",
        "  return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GaNrhEHlIwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('dankmemes_task1_train.csv')\n",
        "embedding = pd.read_csv('dankmemes_task1_train_embeddings.csv', header=None)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvVgi7MjyopT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "### Alternate these three, in order to dervie each feedforward network!\n",
        "\n",
        "# X, y = np.array([embedding[1][i].split() for i in range(1600)]).astype(float), df[['Meme']].values\n",
        "# X, y = get_X(pickle.load(open('alexnet_embeddings.pkl', 'rb'))), df[['Meme']].values\n",
        "# X, y = pickle.load(open('densenest_embeddings.pkl', 'rb')), df[['Meme']].values"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiKt6ID3yoKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "\n",
        "ssc = StandardScaler()\n",
        "mms = MinMaxScaler()"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbllPvbVykuv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "05f5c0e7-0df8-4460-a81f-f8648bc74b5e"
      },
      "source": [
        "import pickle\n",
        "from datetime import date\n",
        "\n",
        "temp = mms.fit_transform(np.array([(date(int(i[0].split('-')[0]), int(i[0].split('-')[1]), int(i[0].split('-')[2])) - date(2015, 1, 1)).days for i in df[['Date']].values.tolist()]).reshape(1600, 1))\n",
        "print(temp.shape)\n",
        "X = np.hstack((X, temp))\n",
        "print(X.shape)\n",
        "\n",
        "temp = ssc.fit_transform(df[['Engagement']].values)\n",
        "print(temp.shape)\n",
        "X = np.hstack((X, temp))\n",
        "print(X.shape)\n",
        "\n",
        "temp = ohe(create_map())\n",
        "print(temp.shape)\n",
        "X = np.hstack((X, temp))\n",
        "print(X.shape)\n",
        "\n",
        "temp = pickle.load(open('cls_umberto_embeddings.pkl', 'rb'))\n",
        "print(temp.shape)\n",
        "X = np.hstack((X, temp))\n",
        "print(X.shape)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1600, 1)\n",
            "(1600, 1001)\n",
            "(1600, 1)\n",
            "(1600, 1002)\n",
            "(1600, 71)\n",
            "(1600, 1073)\n",
            "(1600, 768)\n",
            "(1600, 1841)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1eUkI3H-OeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "buffer_size = 10000\n",
        "batch_size = 64\n",
        "num_epochs = 100"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VFHiDsEy6iQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "af46b0e9-619d-41d7-d6e0-992863d897bc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "# X_train = augment(X_train)\n",
        "# y_train = np.repeat(y_train, 10, axis=0)\n",
        "print(X.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "ds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "ds_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "ds_train = ds_train.shuffle(buffer_size=buffer_size,\n",
        "                            reshuffle_each_iteration=False)\n",
        "ds_test = ds_test.batch(batch_size)\n",
        "ds_train = ds_train.batch(batch_size)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1600, 1841) (1280, 1841) (1280, 1) (320, 1841) (320, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DngRJyN_-nHk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "36245f25-ee04-4357-e4a3-1305b6d500e4"
      },
      "source": [
        "model = build_model(input_shape=(None, 1841)) ## Modify input shape!\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "model.summary()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 1)\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1024)              1886208   \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 2,747,649\n",
            "Trainable params: 2,747,649\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPzpe4i_-sMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "ff3637bd-00a4-4610-deba-0fc5ddd2792b"
      },
      "source": [
        "hist = model.fit(ds_train,\n",
        "                 validation_data=ds_test,\n",
        "                 epochs=num_epochs,\n",
        "                 batch_size=batch_size,\n",
        "                 use_multiprocessing=True,\n",
        "                 callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=5, restore_best_weights=True)])"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "20/20 [==============================] - 1s 38ms/step - loss: 0.6210 - binary_accuracy: 0.6687 - val_loss: 0.4734 - val_binary_accuracy: 0.7688\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 1s 34ms/step - loss: 0.5085 - binary_accuracy: 0.7469 - val_loss: 0.4534 - val_binary_accuracy: 0.7844\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 1s 34ms/step - loss: 0.4725 - binary_accuracy: 0.7836 - val_loss: 0.4874 - val_binary_accuracy: 0.7563\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 1s 34ms/step - loss: 0.4401 - binary_accuracy: 0.7969 - val_loss: 0.4410 - val_binary_accuracy: 0.7875\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 1s 34ms/step - loss: 0.4085 - binary_accuracy: 0.8172 - val_loss: 0.4273 - val_binary_accuracy: 0.8188\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 1s 34ms/step - loss: 0.3918 - binary_accuracy: 0.8281 - val_loss: 0.4097 - val_binary_accuracy: 0.8094\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 1s 33ms/step - loss: 0.3647 - binary_accuracy: 0.8383 - val_loss: 0.4570 - val_binary_accuracy: 0.7812\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 1s 34ms/step - loss: 0.3446 - binary_accuracy: 0.8500 - val_loss: 0.4626 - val_binary_accuracy: 0.8000\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 1s 34ms/step - loss: 0.3319 - binary_accuracy: 0.8531 - val_loss: 0.4251 - val_binary_accuracy: 0.8094\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 1s 34ms/step - loss: 0.2962 - binary_accuracy: 0.8680 - val_loss: 0.4622 - val_binary_accuracy: 0.8188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMk5qUgXSbmh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e7874aaf-c82a-427c-e6a3-ba05e6e9220e"
      },
      "source": [
        "model.save('models/alexnet_ffwd')"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: models/alexnet_ffwd/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5YYD5S-4JVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp -r models ./drive/My\\ Drive/Colab\\ Notebooks/"
      ],
      "execution_count": 111,
      "outputs": []
    }
  ]
}